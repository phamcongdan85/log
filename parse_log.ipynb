{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce07355",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-02T04:09:39.512795Z",
     "iopub.status.busy": "2026-02-02T04:09:39.512380Z",
     "iopub.status.idle": "2026-02-02T04:09:41.280093Z",
     "shell.execute_reply": "2026-02-02T04:09:41.278367Z"
    },
    "papermill": {
     "duration": 1.774995,
     "end_time": "2026-02-02T04:09:41.282907",
     "exception": false,
     "start_time": "2026-02-02T04:09:39.507912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/hdfs-2k/HDFS_2k.log\n",
      "/kaggle/input/hdfs-2k/hdfs_log_templates.json\n",
      "/kaggle/input/hdfs-2k/HDFS_2k.log_structured.csv\n",
      "/kaggle/input/hdfs-2k/HDFS_2k.log_templates.csv\n",
      "/kaggle/input/hdfs-2k/global_sequence.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07dd239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T04:09:41.289792Z",
     "iopub.status.busy": "2026-02-02T04:09:41.289095Z",
     "iopub.status.idle": "2026-02-02T04:09:41.352691Z",
     "shell.execute_reply": "2026-02-02T04:09:41.351491Z"
    },
    "papermill": {
     "duration": 0.070547,
     "end_time": "2026-02-02T04:09:41.355729",
     "exception": false,
     "start_time": "2026-02-02T04:09:41.285182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:353: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:368: SyntaxWarning: invalid escape sequence '\\<'\n",
      "<>:353: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:368: SyntaxWarning: invalid escape sequence '\\<'\n",
      "/tmp/ipykernel_17/2566013903.py:353: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  splitter = re.sub(\" +\", \"\\\\\\s+\", splitters[k])\n",
      "/tmp/ipykernel_17/2566013903.py:368: SyntaxWarning: invalid escape sequence '\\<'\n",
      "  template_regex = \"^\" + template_regex.replace(\"\\<\\*\\>\", \"(.*?)\") + \"$\"\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================\n",
    "# Copyright (C) 2016-2023 LOGPAI (https://github.com/logpai).\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =========================================================================\n",
    "\n",
    "\n",
    "import regex as re\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Logcluster:\n",
    "    def __init__(self, logTemplate=\"\", logIDL=None):\n",
    "        self.logTemplate = logTemplate\n",
    "        if logIDL is None:\n",
    "            logIDL = []\n",
    "        self.logIDL = logIDL\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, childD=None, depth=0, digitOrtoken=None):\n",
    "        if childD is None:\n",
    "            childD = dict()\n",
    "        self.childD = childD\n",
    "        self.depth = depth\n",
    "        self.digitOrtoken = digitOrtoken\n",
    "\n",
    "\n",
    "class LogParser:\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_format,\n",
    "        indir=\"./\",\n",
    "        outdir=\"./result/\",\n",
    "        depth=4,\n",
    "        st=0.4,\n",
    "        maxChild=100,\n",
    "        rex=[],\n",
    "        keep_para=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "            rex : regular expressions used in preprocessing (step1)\n",
    "            path : the input path stores the input log file name\n",
    "            depth : depth of all leaf nodes\n",
    "            st : similarity threshold\n",
    "            maxChild : max number of children of an internal node\n",
    "            logName : the name of the input file containing raw log messages\n",
    "            savePath : the output path stores the file containing structured logs\n",
    "        \"\"\"\n",
    "        self.path = indir\n",
    "        self.depth = depth - 2\n",
    "        self.st = st\n",
    "        self.maxChild = maxChild\n",
    "        self.logName = None\n",
    "        self.savePath = outdir\n",
    "        self.df_log = None\n",
    "        self.log_format = log_format\n",
    "        self.rex = rex\n",
    "        self.keep_para = keep_para\n",
    "\n",
    "    def hasNumbers(self, s):\n",
    "        return any(char.isdigit() for char in s)\n",
    "\n",
    "    def treeSearch(self, rn, seq):\n",
    "        retLogClust = None\n",
    "\n",
    "        seqLen = len(seq)\n",
    "        if seqLen not in rn.childD:\n",
    "            return retLogClust\n",
    "\n",
    "        parentn = rn.childD[seqLen]\n",
    "\n",
    "        currentDepth = 1\n",
    "        for token in seq:\n",
    "            if currentDepth >= self.depth or currentDepth > seqLen:\n",
    "                break\n",
    "\n",
    "            if token in parentn.childD:\n",
    "                parentn = parentn.childD[token]\n",
    "            elif \"<*>\" in parentn.childD:\n",
    "                parentn = parentn.childD[\"<*>\"]\n",
    "            else:\n",
    "                return retLogClust\n",
    "            currentDepth += 1\n",
    "\n",
    "        logClustL = parentn.childD\n",
    "\n",
    "        retLogClust = self.fastMatch(logClustL, seq)\n",
    "\n",
    "        return retLogClust\n",
    "\n",
    "    def addSeqToPrefixTree(self, rn, logClust):\n",
    "        seqLen = len(logClust.logTemplate)\n",
    "        if seqLen not in rn.childD:\n",
    "            firtLayerNode = Node(depth=1, digitOrtoken=seqLen)\n",
    "            rn.childD[seqLen] = firtLayerNode\n",
    "        else:\n",
    "            firtLayerNode = rn.childD[seqLen]\n",
    "\n",
    "        parentn = firtLayerNode\n",
    "\n",
    "        currentDepth = 1\n",
    "        for token in logClust.logTemplate:\n",
    "            # Add current log cluster to the leaf node\n",
    "            if currentDepth >= self.depth or currentDepth > seqLen:\n",
    "                if len(parentn.childD) == 0:\n",
    "                    parentn.childD = [logClust]\n",
    "                else:\n",
    "                    parentn.childD.append(logClust)\n",
    "                break\n",
    "\n",
    "            # If token not matched in this layer of existing tree.\n",
    "            if token not in parentn.childD:\n",
    "                if not self.hasNumbers(token):\n",
    "                    if \"<*>\" in parentn.childD:\n",
    "                        if len(parentn.childD) < self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth + 1, digitOrtoken=token)\n",
    "                            parentn.childD[token] = newNode\n",
    "                            parentn = newNode\n",
    "                        else:\n",
    "                            parentn = parentn.childD[\"<*>\"]\n",
    "                    else:\n",
    "                        if len(parentn.childD) + 1 < self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth + 1, digitOrtoken=token)\n",
    "                            parentn.childD[token] = newNode\n",
    "                            parentn = newNode\n",
    "                        elif len(parentn.childD) + 1 == self.maxChild:\n",
    "                            newNode = Node(depth=currentDepth + 1, digitOrtoken=\"<*>\")\n",
    "                            parentn.childD[\"<*>\"] = newNode\n",
    "                            parentn = newNode\n",
    "                        else:\n",
    "                            parentn = parentn.childD[\"<*>\"]\n",
    "\n",
    "                else:\n",
    "                    if \"<*>\" not in parentn.childD:\n",
    "                        newNode = Node(depth=currentDepth + 1, digitOrtoken=\"<*>\")\n",
    "                        parentn.childD[\"<*>\"] = newNode\n",
    "                        parentn = newNode\n",
    "                    else:\n",
    "                        parentn = parentn.childD[\"<*>\"]\n",
    "\n",
    "            # If the token is matched\n",
    "            else:\n",
    "                parentn = parentn.childD[token]\n",
    "\n",
    "            currentDepth += 1\n",
    "\n",
    "    # seq1 is template\n",
    "    def seqDist(self, seq1, seq2):\n",
    "        assert len(seq1) == len(seq2)\n",
    "        simTokens = 0\n",
    "        numOfPar = 0\n",
    "\n",
    "        for token1, token2 in zip(seq1, seq2):\n",
    "            if token1 == \"<*>\":\n",
    "                numOfPar += 1\n",
    "                continue\n",
    "            if token1 == token2:\n",
    "                simTokens += 1\n",
    "\n",
    "        retVal = float(simTokens) / len(seq1)\n",
    "\n",
    "        return retVal, numOfPar\n",
    "\n",
    "    def fastMatch(self, logClustL, seq):\n",
    "        retLogClust = None\n",
    "\n",
    "        maxSim = -1\n",
    "        maxNumOfPara = -1\n",
    "        maxClust = None\n",
    "\n",
    "        for logClust in logClustL:\n",
    "            curSim, curNumOfPara = self.seqDist(logClust.logTemplate, seq)\n",
    "            if curSim > maxSim or (curSim == maxSim and curNumOfPara > maxNumOfPara):\n",
    "                maxSim = curSim\n",
    "                maxNumOfPara = curNumOfPara\n",
    "                maxClust = logClust\n",
    "\n",
    "        if maxSim >= self.st:\n",
    "            retLogClust = maxClust\n",
    "\n",
    "        return retLogClust\n",
    "\n",
    "    def getTemplate(self, seq1, seq2):\n",
    "        assert len(seq1) == len(seq2)\n",
    "        retVal = []\n",
    "\n",
    "        i = 0\n",
    "        for word in seq1:\n",
    "            if word == seq2[i]:\n",
    "                retVal.append(word)\n",
    "            else:\n",
    "                retVal.append(\"<*>\")\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return retVal\n",
    "\n",
    "    def outputResult(self, logClustL):\n",
    "        log_templates = [0] * self.df_log.shape[0]\n",
    "        log_templateids = [0] * self.df_log.shape[0]\n",
    "        df_events = []\n",
    "        for logClust in logClustL:\n",
    "            template_str = \" \".join(logClust.logTemplate)\n",
    "            occurrence = len(logClust.logIDL)\n",
    "            template_id = hashlib.md5(template_str.encode(\"utf-8\")).hexdigest()[0:8]\n",
    "            for logID in logClust.logIDL:\n",
    "                logID -= 1\n",
    "                log_templates[logID] = template_str\n",
    "                log_templateids[logID] = template_id\n",
    "            df_events.append([template_id, template_str, occurrence])\n",
    "\n",
    "        df_event = pd.DataFrame(\n",
    "            df_events, columns=[\"EventId\", \"EventTemplate\", \"Occurrences\"]\n",
    "        )\n",
    "        self.df_log[\"EventId\"] = log_templateids\n",
    "        self.df_log[\"EventTemplate\"] = log_templates\n",
    "        if self.keep_para:\n",
    "            self.df_log[\"ParameterList\"] = self.df_log.apply(\n",
    "                self.get_parameter_list, axis=1\n",
    "            )\n",
    "        self.df_log.to_csv(\n",
    "            os.path.join(self.savePath, self.logName + \"_structured.csv\"), index=False\n",
    "        )\n",
    "\n",
    "        occ_dict = dict(self.df_log[\"EventTemplate\"].value_counts())\n",
    "        df_event = pd.DataFrame()\n",
    "        df_event[\"EventTemplate\"] = self.df_log[\"EventTemplate\"].unique()\n",
    "        df_event[\"EventId\"] = df_event[\"EventTemplate\"].map(\n",
    "            lambda x: hashlib.md5(x.encode(\"utf-8\")).hexdigest()[0:8]\n",
    "        )\n",
    "        df_event[\"Occurrences\"] = df_event[\"EventTemplate\"].map(occ_dict)\n",
    "        df_event.to_csv(\n",
    "            os.path.join(self.savePath, self.logName + \"_templates.csv\"),\n",
    "            index=False,\n",
    "            columns=[\"EventId\", \"EventTemplate\", \"Occurrences\"],\n",
    "        )\n",
    "\n",
    "    def printTree(self, node, dep):\n",
    "        pStr = \"\"\n",
    "        for i in range(dep):\n",
    "            pStr += \"\\t\"\n",
    "\n",
    "        if node.depth == 0:\n",
    "            pStr += \"Root\"\n",
    "        elif node.depth == 1:\n",
    "            pStr += \"<\" + str(node.digitOrtoken) + \">\"\n",
    "        else:\n",
    "            pStr += node.digitOrtoken\n",
    "\n",
    "        print(pStr)\n",
    "\n",
    "        if node.depth == self.depth:\n",
    "            return 1\n",
    "        for child in node.childD:\n",
    "            self.printTree(node.childD[child], dep + 1)\n",
    "\n",
    "    def parse(self, logName):\n",
    "        print(\"Parsing file: \" + os.path.join(self.path, logName))\n",
    "        start_time = datetime.now()\n",
    "        self.logName = logName\n",
    "        rootNode = Node()\n",
    "        logCluL = []\n",
    "\n",
    "        self.load_data()\n",
    "\n",
    "        count = 0\n",
    "        for idx, line in self.df_log.iterrows():\n",
    "            logID = line[\"LineId\"]\n",
    "            logmessageL = self.preprocess(line[\"Content\"]).strip().split()\n",
    "            matchCluster = self.treeSearch(rootNode, logmessageL)\n",
    "\n",
    "            # Match no existing log cluster\n",
    "            if matchCluster is None:\n",
    "                newCluster = Logcluster(logTemplate=logmessageL, logIDL=[logID])\n",
    "                logCluL.append(newCluster)\n",
    "                self.addSeqToPrefixTree(rootNode, newCluster)\n",
    "\n",
    "            # Add the new log message to the existing cluster\n",
    "            else:\n",
    "                newTemplate = self.getTemplate(logmessageL, matchCluster.logTemplate)\n",
    "                matchCluster.logIDL.append(logID)\n",
    "                if \" \".join(newTemplate) != \" \".join(matchCluster.logTemplate):\n",
    "                    matchCluster.logTemplate = newTemplate\n",
    "\n",
    "            count += 1\n",
    "            if count % 1000 == 0 or count == len(self.df_log):\n",
    "                print(\n",
    "                    \"Processed {0:.1f}% of log lines.\".format(\n",
    "                        count * 100.0 / len(self.df_log)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        if not os.path.exists(self.savePath):\n",
    "            os.makedirs(self.savePath)\n",
    "\n",
    "        self.outputResult(logCluL)\n",
    "\n",
    "        print(\"Parsing done. [Time taken: {!s}]\".format(datetime.now() - start_time))\n",
    "\n",
    "    def load_data(self):\n",
    "        headers, regex = self.generate_logformat_regex(self.log_format)\n",
    "        self.df_log = self.log_to_dataframe(\n",
    "            os.path.join(self.path, self.logName), regex, headers, self.log_format\n",
    "        )\n",
    "\n",
    "    def preprocess(self, line):\n",
    "        for currentRex in self.rex:\n",
    "            line = re.sub(currentRex, \"<*>\", line)\n",
    "        return line\n",
    "\n",
    "    def log_to_dataframe(self, log_file, regex, headers, logformat):\n",
    "        \"\"\"Function to transform log file to dataframe\"\"\"\n",
    "        log_messages = []\n",
    "        linecount = 0\n",
    "        with open(log_file, \"r\") as fin:\n",
    "            for line in fin.readlines():\n",
    "                try:\n",
    "                    match = regex.search(line.strip())\n",
    "                    message = [match.group(header) for header in headers]\n",
    "                    log_messages.append(message)\n",
    "                    linecount += 1\n",
    "                except Exception as e:\n",
    "                    print(\"[Warning] Skip line: \" + line)\n",
    "        logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "        logdf.insert(0, \"LineId\", None)\n",
    "        logdf[\"LineId\"] = [i + 1 for i in range(linecount)]\n",
    "        print(\"Total lines: \", len(logdf))\n",
    "        return logdf\n",
    "\n",
    "    def generate_logformat_regex(self, logformat):\n",
    "        \"\"\"Function to generate regular expression to split log messages\"\"\"\n",
    "        headers = []\n",
    "        splitters = re.split(r\"(<[^<>]+>)\", logformat)\n",
    "        regex = \"\"\n",
    "        for k in range(len(splitters)):\n",
    "            if k % 2 == 0:\n",
    "                splitter = re.sub(\" +\", \"\\\\\\s+\", splitters[k])\n",
    "                regex += splitter\n",
    "            else:\n",
    "                header = splitters[k].strip(\"<\").strip(\">\")\n",
    "                regex += \"(?P<%s>.*?)\" % header\n",
    "                headers.append(header)\n",
    "        regex = re.compile(\"^\" + regex + \"$\")\n",
    "        return headers, regex\n",
    "\n",
    "    def get_parameter_list(self, row):\n",
    "        template_regex = re.sub(r\"<.{1,5}>\", \"<*>\", row[\"EventTemplate\"])\n",
    "        if \"<*>\" not in template_regex:\n",
    "            return []\n",
    "        template_regex = re.sub(r\"([^A-Za-z0-9])\", r\"\\\\\\1\", template_regex)\n",
    "        template_regex = re.sub(r\"\\\\ +\", r\"\\\\s+\", template_regex)\n",
    "        template_regex = \"^\" + template_regex.replace(\"\\<\\*\\>\", \"(.*?)\") + \"$\"\n",
    "        parameter_list = re.findall(template_regex, row[\"Content\"])\n",
    "        parameter_list = parameter_list[0] if parameter_list else ()\n",
    "        parameter_list = (\n",
    "            list(parameter_list)\n",
    "            if isinstance(parameter_list, tuple)\n",
    "            else [parameter_list]\n",
    "        )\n",
    "        return parameter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55320d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T04:09:41.363837Z",
     "iopub.status.busy": "2026-02-02T04:09:41.362693Z",
     "iopub.status.idle": "2026-02-02T04:09:41.751338Z",
     "shell.execute_reply": "2026-02-02T04:09:41.749461Z"
    },
    "papermill": {
     "duration": 0.396194,
     "end_time": "2026-02-02T04:09:41.754216",
     "exception": false,
     "start_time": "2026-02-02T04:09:41.358022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file: /kaggle/input/hdfs-2k/HDFS_2k.log\n",
      "Total lines:  2000\n",
      "Processed 50.0% of log lines.\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.369134]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "# from logparser.logparser import Drain\n",
    "\n",
    "RAW_DIR = \"/kaggle/input/hdfs-2k/\"\n",
    "OUT_DIR = \"data/structured/\"\n",
    "LOG_FILE = \"HDFS_2k.log\"\n",
    "\n",
    "LOG_FORMAT = \"<Date> <Time> <Pid> <Level> <Component>: <Content>\"\n",
    "\n",
    "REGEX = [\n",
    "    r\"(?<=blk_)[-\\d]+\",        # block id\n",
    "    r\"\\d+\\.\\d+\\.\\d+\\.\\d+\",     # IP\n",
    "    r\"(/[-\\w]+)+\",             # file path\n",
    "]\n",
    "\n",
    "def parse_hdfs():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    parser = LogParser(\n",
    "        log_format=LOG_FORMAT,\n",
    "        indir=RAW_DIR,\n",
    "        outdir=OUT_DIR,\n",
    "        depth=5,\n",
    "        st=0.5,\n",
    "        rex=REGEX,\n",
    "        keep_para=False\n",
    "    )\n",
    "    parser.parse(LOG_FILE)\n",
    "\n",
    "def build_logkey_mapping():\n",
    "    tpl = pd.read_csv(f\"{OUT_DIR}/{LOG_FILE}_templates.csv\")\n",
    "    tpl.sort_values(\"Occurrences\", ascending=False, inplace=True)\n",
    "\n",
    "    mapping = {\n",
    "        eid: idx + 1\n",
    "        for idx, eid in enumerate(tpl[\"EventId\"])\n",
    "    }\n",
    "\n",
    "    with open(f\"{OUT_DIR}/hdfs_log_templates.json\", \"w\") as f:\n",
    "        json.dump(mapping, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parse_hdfs()\n",
    "    build_logkey_mapping()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 9364016,
     "sourceId": 14670123,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.540671,
   "end_time": "2026-02-02T04:09:42.385253",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-02T04:09:34.844582",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
